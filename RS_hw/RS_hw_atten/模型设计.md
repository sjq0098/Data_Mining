

## 一、单塔框架


**方式**：

* **“用户–物品”塔**，即把评分矩阵视作一个二部图 $G_r=(U\cup V, E_r)$，用图卷积网络（GCN/GAT/LightGCN 等）对这个二部图做多跳信息聚合。这里我选择使用LightGCN适用于稀疏数据。

```text
用户–物品塔：
  输入：稀疏评分矩阵 R
  模型：多层 GCN/GAT/LightGCN
  输出：用户表示 P_r ∈ ℝ^{|U|×d}, 物品表示 Q ∈ ℝ^{|V|×d}
预测层：
  输入：P_r(u), Q(i)
  结构：MLP 或 点积
  输出：打分
```

---

## 二、图增强 VAE（Graph-Enhanced VAE）

在 GCN 之外，用层级化的 VAE 对节点表征做分布式编码和重构，既做降噪又做正则化。
**方式**：

* **做法**：每一跳 GCN 输出 $H^{(l)}$ 都作为 VAE 的输入，学习高斯分布 $\mathcal{N}(\mu^{(l)},\sigma^{(l)2})$，然后重构回 $H^{(l)}$，在 ELBO 损失里加上重构误差＋KL 散度。
* **好处**：

  1. **抗噪性**：对极度稀疏的评分图做隐式平滑。
  2. **正则化**：抑制过拟合，让多跳信息更稳定。

---

## 三、层级注意力（Layerwise Graph Attention）

**借鉴方式**：

* 对 Bipartite Path 的每一层 GCN 输出 $H^{(l)}$，不必简单地平均或逐层残差相加，而是引入一个 **层级注意力权重** $\alpha_l$：

  $$
    P_r = \sum_{l=0}^L \alpha_l \, H^{(l)},\quad
    \alpha = \mathrm{softmax}(W\,[H^{(0)};\dots;H^{(L)}])
  $$
* **好处**：模型能自适应地聚焦那些对最终任务最有贡献的跳数（短程 vs. 长程交互）。

---

## 四、双重采样（Dual Sampling）策略

**论文亮点**：用 Gumbel-Softmax + Bernoulli 双重采样稳定图结构，防止邻居选择过于稠密或过于稀疏。
**借鉴方式**：

* 在用户–物品图中，对每个节点的邻居（评分过的物品或评分用户）进行：

  1. **Gumbel-Softmax 软选择**，强调“重要”连接；
  2. **Bernoulli 抽样**，控制平均度数。
* **好处**：

  * 避免大度节点（超活跃用户、大众热门物品）主导表示；
  * 保证训练时的计算效率和图结构稳定性。

---

## 五、端到端联合优化

**论文思路**：主任务（预测评分或排序）＋VAE 重构损失 ＋采样过程的连续松弛项，一起反向传播。
**借鉴方式**：

* 直接把 **ELBO（重构＋KL）损失** 和 **预测损失**（BPR、MSE、交叉熵等）加权相加：

  $$
    \mathcal{L} = \mathcal{L}_\text{pred} + \lambda_\text{VAE}\,\mathcal{L}_\text{ELBO}
  $$
* 训练时同时更新 GCN 层、VAE 编码器／解码器、采样参数，保持端到端一致性。

---

总之，端到端，读入train.txt分文训练集和验证集进行训练和调优，返回优化RMSE损失，对text.txt只进行预测输出
